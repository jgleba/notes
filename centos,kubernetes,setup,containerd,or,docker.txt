#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@#@  #@  CENTOS KUBERNETES CLUSTER SETUP + CONTAINERD OR DOCKER + METALLB AND TRAEFIK INGRESS#@  #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@   2021-05-28[May-Fri]15-20PM ==GLOBAL PRE-FLIGHT==sudo yum update -ySWAP OFF ON VM LEVELsu root==FIREWALL OFF ON ALL MASTERS AND WORKERS - NOT WORKING WITHOUT==sudo systemctl stop firewalld && sudo systemctl disable firewalld==HOSTS FILE ON ALL==sudo nano /etc/hosts172.16.3.160 kube1master1172.16.3.161 kube1worker1172.16.3.162 kube1worker2172.16.3.163 kube2master1172.16.3.164 kube2worker1172.16.3.165 kube2worker2==INSTALL CLIENT TOOLS ON MASTER1==sudo wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64sudo wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64sudo chmod +x cfssl*sudo mv cfssl_linux-amd64 /usr/local/bin/cfsslsudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson==VERIFY VERSION==cfssl version==DOWNLOAD AND INSTALL LATEST KUBECTL - ON ALL MACHINES WHERE MANAGEMENT MIGHT HAPPEN - HAPROXY1, 2 MASTER1, 2, 3==##https://kubernetes.io/docs/tasks/tools/install-kubectl/sudo curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectlsudo chmod +x kubectlsudo mv kubectl /usr/local/bin==VERIFY VERSION==kubectl versionORkubectl version -o jsonNOT SURE IF ERROR BELOW MATTERS OR NOT[root@kube1haproxy1 user]# kubectl versionClient Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:23:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}The connection to the server localhost:8080 was refused - did you specify the right host or port?[root@kube1haproxy1 user]# kubectl version -o json{  "clientVersion": {    "major": "1",    "minor": "16",    "gitVersion": "v1.16.3",    "gitCommit": "b3cbbae08ec52a7fc73d334838e18d17e8512749",    "gitTreeState": "clean",    "buildDate": "2019-11-13T11:23:11Z",    "goVersion": "go1.12.12",    "compiler": "gc",    "platform": "linux/amd64"  }}The connection to the server localhost:8080 was refused - did you specify the right host or port?CONTINUE==CREATE CA ON MASTER1==sudo nano ca-config.json{  "signing": {    "default": {      "expiry": "87600h"    },    "profiles": {      "kubernetes": {        "usages": ["signing", "key encipherment", "server auth", "client auth"],        "expiry": "87600h"      }    }  }}sudo nano ca-csr.json{  "CN": "JKubeCA",  "key": {    "algo": "rsa",    "size": 2048  },  "names": [  {    "C": "JKube",    "L": "JKube",    "O": "JKube",    "OU": "JKube",    "ST": "JKube"  } ]}==GENERATE CERTS AND KEYS FOR CA==cfssl gencert -initca ca-csr.json | cfssljson -bare ca==CREATE CA FOR ETCD CLUSTER==sudo nano kubernetes-csr.json{  "CN": "JKube",  "key": {    "algo": "rsa",    "size": 2048  },  "names": [  {    "C": "JKube",    "L": "JKube",    "O": "JKube",    "OU": "JKube",    "ST": "JKube"  } ]}==GENERATE CERTS AND KEYS==NEEDS TO BE UPDATED ON NEXT REBUILDcfssl gencert \-ca=ca.pem \-ca-key=ca-key.pem \-config=ca-config.json \-hostname=172.16.3.160,172.16.3.161,172.16.3.162,127.0.0.1,kubernetes.default \-profile=kubernetes kubernetes-csr.json | \cfssljson -bare kubernetescfssl gencert \-ca=ca.pem \-ca-key=ca-key.pem \-config=ca-config.json \-hostname=172.16.3.163,172.16.3.164,172.16.3.165,127.0.0.1,kubernetes.default \-profile=kubernetes kubernetes-csr.json | \cfssljson -bare kubernetes==COPY CERTS TO ALL MACHINES==NEEDS TO BE UPDATED ON NEXT REBUILDscp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.160:/root/scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.161:/root/scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.162:/root/scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.163:/root/scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.164:/root/scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.3.165:/root/==INSTALL DOCKER OR CONTAINERD, CONTAINERD BELOW==sudo yum install docker -ysudo systemctl enable docker && sudo systemctl start docker==INSTALL CONTAINERD ON ALL MASTERS AND WORKERS==##https://computingforgeeks.com/install-kubernetes-cluster-on-centos-with-kubeadm/# Configure persistent loading of modulessudo tee /etc/modules-load.d/containerd.conf <<EOFoverlaybr_netfilterEOF# Load at runtimesudo modprobe overlaysudo modprobe br_netfilter# Ensure sysctl params are setsudo tee /etc/sysctl.d/kubernetes.conf<<EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1EOF# Reload configssudo sysctl --system# Install required packagessudo yum install -y yum-utils device-mapper-persistent-data lvm2# Add Docker reposudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# Install containerdsudo yum update -y && sudo yum install -y containerd.io# Configure containerd and start servicesudo su rootsudo mkdir -p /etc/containerdsudo containerd config default > /etc/containerd/config.toml#sudo tee /etc/containerd/config.toml>>EOFexit# restart containerdsudo systemctl restart containerdsudo systemctl enable containerd==INSTALL kubelet kubeadm kubectl ON ALL MASTERS AND WORKERS==echo "[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg" | sudo tee /etc/yum.repos.d/kubernetes.reposudo yum install -y kubelet kubeadm kubectlsudo systemctl enable kubelet.service && sudo systemctl start kubelet.service==ETCD CLUSTER INSTALL ON ALL MASTERS==sudo mkdir /etc/etcd /var/lib/etcdsudo cp /root/ca.pem /root/kubernetes.pem /root/kubernetes-key.pem /etc/etcdsudo cp /home/user/ca.pem /home/user/kubernetes.pem /home/user/kubernetes-key.pem /etc/etcdsudo curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest \  | grep browser_download_url \  | grep linux-amd64 \  | cut -d '"' -f 4 \  | wget -qi -  sudo tar xvf *.tar.gzcd etcd-*/sudo mv etcd* /usr/local/bin/cd ..sudo rm -rf etcd*==MASTER1 CREATE ETCD CONFIG FILE==sudo su rootecho '[Unit]Description=etcdDocumentation=https://github.com/coreos[Service]ExecStart=/usr/local/bin/etcd \  --name 172.16.3.160 \  --cert-file=/etc/etcd/kubernetes.pem \  --key-file=/etc/etcd/kubernetes-key.pem \  --peer-cert-file=/etc/etcd/kubernetes.pem \  --peer-key-file=/etc/etcd/kubernetes-key.pem \  --trusted-ca-file=/etc/etcd/ca.pem \  --peer-trusted-ca-file=/etc/etcd/ca.pem \  --peer-client-cert-auth \  --client-cert-auth \  --initial-advertise-peer-urls https://172.16.3.160:2380 \  --listen-peer-urls https://172.16.3.160:2380 \  --listen-client-urls https://172.16.3.160:2379,http://127.0.0.1:2379 \  --advertise-client-urls https://172.16.3.160:2379 \  --initial-cluster-token etcd-cluster-0 \  --initial-cluster 172.16.3.160=https://172.16.3.160:2380 \  --initial-cluster-state new \  --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.target' > /etc/systemd/system/etcd.service==echo '[Unit]Description=etcdDocumentation=https://github.com/coreos[Service]ExecStart=/usr/local/bin/etcd \  --name 172.16.3.163 \  --cert-file=/etc/etcd/kubernetes.pem \  --key-file=/etc/etcd/kubernetes-key.pem \  --peer-cert-file=/etc/etcd/kubernetes.pem \  --peer-key-file=/etc/etcd/kubernetes-key.pem \  --trusted-ca-file=/etc/etcd/ca.pem \  --peer-trusted-ca-file=/etc/etcd/ca.pem \  --peer-client-cert-auth \  --client-cert-auth \  --initial-advertise-peer-urls https://172.16.3.163:2380 \  --listen-peer-urls https://172.16.3.163:2380 \  --listen-client-urls https://172.16.3.163:2379,http://127.0.0.1:2379 \  --advertise-client-urls https://172.16.3.163:2379 \  --initial-cluster-token etcd-cluster-0 \  --initial-cluster 172.16.3.163=https://172.16.3.163:2380 \  --initial-cluster-state new \  --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.target' > /etc/systemd/system/etcd.service==ENABLE AND START ETCD==sudo systemctl daemon-reload && sudo systemctl enable etcd && sudo systemctl restart etcd==INITALIZING MASTER NODES, MASTER1 CONFIG==echo 'apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: stableapiServerCertSANs:- 172.16.3.160controlPlaneEndpoint: "172.16.3.160:6443"etcd:  external:    endpoints:    - https://172.16.3.160:2379    caFile: /etc/etcd/ca.pem    certFile: /etc/etcd/kubernetes.pem    keyFile: /etc/etcd/kubernetes-key.pemnetworking:  podSubnet: 10.252.0.0/16apiServerExtraArgs:  apiserver-count: "3"' > config.yaml  ==echo 'apiVersion: kubeadm.k8s.io/v1beta2kind: ClusterConfigurationkubernetesVersion: stableapiServerCertSANs:- 172.16.3.163controlPlaneEndpoint: "172.16.3.163:6443"etcd:  external:    endpoints:    - https://172.16.3.163:2379    caFile: /etc/etcd/ca.pem    certFile: /etc/etcd/kubernetes.pem    keyFile: /etc/etcd/kubernetes-key.pemnetworking:  podSubnet: 10.252.0.0/16apiServerExtraArgs:  apiserver-count: "3"' > config.yaml  ==INIT CLUSTER ON MASTER1==sudo kubeadm init --config=config.yaml==DONE, CONTROL PANE RUNNING, JOIN NODES==Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:  export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root:  kubeadm join 172.16.3.160:6443 --token sdki59.hlryt3bo58nqig16 \        --discovery-token-ca-cert-hash sha256:f9aa8d1f8f0248ac84e1e02f62bb7468f8ebe8f69ae1524a894d535a73422dc0 \        --control-planeThen you can join any number of worker nodes by running the following on each as root:kubeadm join 172.16.3.160:6443 --token sdki59.hlryt3bo58nqig16 \        --discovery-token-ca-cert-hash sha256:f9aa8d1f8f0248ac84e1e02f62bb7468f8ebe8f69ae1524a894d535a73422dc0==Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:  export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of control-plane nodes by copying certificate authoritiesand service account keys on each node and then running the following as root:  kubeadm join 172.16.3.163:6443 --token jgskvq.8bx4wk1imkc5l61q \        --discovery-token-ca-cert-hash sha256:51beefd81d8b98d24a5125b9675c3b5364978206088305953446a2d9ec0fa373 \        --control-planeThen you can join any number of worker nodes by running the following on each as root:kubeadm join 172.16.3.163:6443 --token jgskvq.8bx4wk1imkc5l61q \        --discovery-token-ca-cert-hash sha256:51beefd81d8b98d24a5125b9675c3b5364978206088305953446a2d9ec0fa373		==JOIN WORKER NODES==sudo kubeadm join 172.16.3.160:6443 --token sdki59.hlryt3bo58nqig16 \        --discovery-token-ca-cert-hash sha256:f9aa8d1f8f0248ac84e1e02f62bb7468f8ebe8f69ae1524a894d535a73422dc0sudo kubeadm join 172.16.3.160:6443 --token d0rfqs.zugcfcc352blclpy \        --discovery-token-ca-cert-hash sha256:d927a921abaefff5881c3a603ab445ae1afcc8422ba55f334fec21f40822fa9bsudo kubeadm join 172.16.3.160:6443 --token 45boxl.vw88x70ud9pbn0ev \        --discovery-token-ca-cert-hash sha256:3f30f6a7f041607e00e53ea09602dc0bcfebb66854dd4d26066347c455832565sudo kubeadm join 172.16.3.160:6443 --token orrndc.bgtvduo6he9fnjkb \        --discovery-token-ca-cert-hash sha256:e1d842a36310cb6892bef6f317c0cd0049da38dab2d23f7a32103e471ae0302b		==CONNECTING TO CLUSTER==mkdir /home/user/.kubesudo cp /etc/kubernetes/admin.conf /home/user/.kube/configsudo chown -R user:user /home/user/.kube/configsudo chmod 600 /home/user/.kube/config==CHECK CLUSTER==kubectl get nodes==DEPLOY OVERLAY NETWORK - TRYING WEAVE / WEAVE NETON MASTER1 - OR MANAGEMENT MACHINE==kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"==CHECK ALL==[user@KUBE1MASTER1 ~]$ kubectl get all -ANAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGEkube-system   pod/coredns-558bd4d5db-c8ndv               1/1     Running   0          14mkube-system   pod/coredns-558bd4d5db-mm4sn               1/1     Running   0          14mkube-system   pod/kube-apiserver-kube1master1            1/1     Running   2          14mkube-system   pod/kube-controller-manager-kube1master1   1/1     Running   2          14mkube-system   pod/kube-proxy-92f9c                       1/1     Running   0          13mkube-system   pod/kube-proxy-j7726                       1/1     Running   0          14mkube-system   pod/kube-proxy-wspnq                       1/1     Running   0          13mkube-system   pod/kube-scheduler-kube1master1            1/1     Running   2          14mkube-system   pod/weave-net-cbslc                        2/2     Running   1          3m6skube-system   pod/weave-net-npf4l                        2/2     Running   1          3m6skube-system   pod/weave-net-sbn9z                        2/2     Running   1          3m6sNAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGEdefault       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  15mkube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   15mNAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGEkube-system   daemonset.apps/kube-proxy   3         3         3       3            3           kubernetes.io/os=linux   15mkube-system   daemonset.apps/weave-net    3         3         3       3            3           <none>                   3m6sNAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGEkube-system   deployment.apps/coredns   2/2     2            2           15mNAMESPACE     NAME                                 DESIRED   CURRENT   READY   AGEkube-system   replicaset.apps/coredns-558bd4d5db   2         2         2       14m==METAL LB INSTALL==kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml[user@kube1haproxy1 ~]$ kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yamlnamespace/metallb-system unchangedpodsecuritypolicy.policy/speaker createdserviceaccount/controller unchangedserviceaccount/speaker unchangedclusterrole.rbac.authorization.k8s.io/metallb-system:controller unchangedclusterrole.rbac.authorization.k8s.io/metallb-system:speaker configuredrole.rbac.authorization.k8s.io/config-watcher configuredclusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller unchangedclusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker unchangedrolebinding.rbac.authorization.k8s.io/config-watcher unchangeddaemonset.apps/speaker createddeployment.apps/controller created==METALLB CONFIG==https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/example-layer2-config.yaml#apiVersion: v1#kind: ConfigMap#metadata:#  namespace: metallb-system#  name: config#data:#  config: |#    address-pools:#    - name: my-ip-space#      protocol: layer2#      addresses:#      - 192.168.1.240/28	 sudo nano metallbconfig.yamlapiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: vlan1723      protocol: layer2      addresses:      - 172.16.3.144/28==  	  echo 'apiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: vlan1723      protocol: layer2      addresses:      - 172.16.3.144/28' > metallbconfig.yaml==sudo nano metallbconfig.yamlapiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: vlan1723      protocol: layer2      addresses:      - 172.16.3.170/29	  ==DEPLOY METALLB CONFIG==kubectl apply -f metallbconfig.yaml==TRAEFIK INGRESS INSTALL==sudo nano traefik-rbac.yaml---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: traefik-ingress-controllerrules:  - apiGroups:      - ""    resources:      - services      - endpoints      - secrets    verbs:      - get      - list      - watch  - apiGroups:      - extensions    resources:      - ingresses    verbs:      - get      - list      - watch  - apiGroups:    - extensions    resources:    - ingresses/status    verbs:    - update---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:  name: traefik-ingress-controllerroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: traefik-ingress-controllersubjects:- kind: ServiceAccount  name: traefik-ingress-controller  namespace: kube-system==  sudo nano traefik-deployment.yaml---apiVersion: v1kind: ServiceAccountmetadata:  name: traefik-ingress-controller  namespace: kube-system---kind: DeploymentapiVersion: apps/v1metadata:  name: traefik-ingress-controller  namespace: kube-system  labels:    k8s-app: traefik-ingress-lbspec:  replicas: 2  selector:    matchLabels:      k8s-app: traefik-ingress-lb  template:    metadata:      labels:        k8s-app: traefik-ingress-lb        name: traefik-ingress-lb    spec:      serviceAccountName: traefik-ingress-controller      terminationGracePeriodSeconds: 60      containers:      - image: traefik:v1.7        name: traefik-ingress-lb        ports:        - name: http          containerPort: 80        - name: admin          containerPort: 8080        args:        - --api        - --kubernetes        - --logLevel=INFO---kind: ServiceapiVersion: v1metadata:  name: traefik-ingress-service  namespace: kube-systemspec:  selector:    k8s-app: traefik-ingress-lb  ports:    - protocol: TCP      port: 80      name: web    - protocol: TCP      port: 8080      name: admin  type: LoadBalancer  ==APPLY TRAEFIK CONFIG==kubectl apply -f traefik-rbac.yamlkubectl apply -f traefik-deployment.yaml